{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [https://groups.google.com/forum/#!searchin/mtadeveloperresources/turnstile\\$20december\\$202014/mtadeveloperresources/9iuZg8ek9mE/amsfZ_OdHk0J](https://groups.google.com/forum/#!searchin/mtadeveloperresources/turnstile$20december$202014/mtadeveloperresources/9iuZg8ek9mE/amsfZ_OdHk0J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To handle I/O:\n",
    "import os\n",
    "\n",
    "# To handle dates:\n",
    "from datetime import date,time,datetime,timedelta\n",
    "\n",
    "# To handle data:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To create progress bars:\n",
    "from tqdm import tqdm_notebook as tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def delta_ddhhmmss(delta):\n",
    "    t = delta.total_seconds()\n",
    "    sign = \"-\" if t<0 else \"+\"\n",
    "    t = -t if t<0 else t\n",
    "    ss = int(t%60)\n",
    "    mm = int(t%(60*60)//(60))\n",
    "    hh = int(t%(24*60*60)//(60*60))\n",
    "    dd = int(t//(24*60*60))\n",
    "    dd = \"{}d \".format(dd) if dd!=0 else \"\"\n",
    "    return \"{} {}{:0>2}h {:0>2}m {:0>2}s\".format(sign,dd,hh,mm,ss).replace(' ','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_outliers(data,grp_cols,data_cols):\n",
    "    \"\"\"\n",
    "        Identifies values that are more than 2 standard deviations from the mean \n",
    "        (in their respective groups) and replaces them with np.nan .\n",
    "    \"\"\"\n",
    "    \n",
    "    #df = data[grp_cols+data_cols].copy()\n",
    "    df = data.copy()\n",
    "    \n",
    "    ## Make all values positive:\n",
    "    #for col in data_cols:\n",
    "    #    df[col] = np.abs(df[col])\n",
    "    \n",
    "    progress_bar = tqdm(total=len(df.drop_duplicates(grp_cols)),desc=\"Remove outliers\")\n",
    "    \n",
    "    new_df = []\n",
    "    \n",
    "    for i,grp in df.groupby(grp_cols,as_index=False):\n",
    "        \n",
    "        grp = grp.copy()\n",
    "        \n",
    "        for col in data_cols:\n",
    "            \n",
    "            mu = grp[col].mean()\n",
    "            sd = grp[col].std()\n",
    "\n",
    "            grp.loc[ (grp[col]<(mu-2*sd)) , col] = np.nan\n",
    "            grp.loc[ (grp[col]>(mu+2*sd)) , col] = np.nan\n",
    "            \n",
    "        new_df.append(grp)\n",
    "        \n",
    "        progress_bar.update()\n",
    "        \n",
    "    progress_bar.close()\n",
    "    \n",
    "    new_df = pd.concat(new_df,axis=0)\n",
    "    \n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lookups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookup_groupings = pd.read_csv('lookups/lookup_groupings.csv')\n",
    "lookup_stations = pd.read_csv('lookups/lookup_stations.csv')\n",
    "lookup_booths = pd.read_csv('lookups/lookup_booths.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookup_booths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "source = []\n",
    "for filename in os.listdir('data/'):\n",
    "    if (filename.find('mta-turnstile-data_')>-1) and (filename.find('.csv')>-1):\n",
    "        date_string = filename.replace('mta-turnstile-data_','').replace('.csv','')\n",
    "        dt = datetime.strptime(date_string,'%Y-%m-%d').date()\n",
    "        style = 'pre-2014-10-18' if dt<datetime(2014,10,18).date() else 'starting-2014-10-18'\n",
    "        filepath = 'data/'+filename\n",
    "        source.append({'dt':dt,'style':style,'filepath':filepath})\n",
    "source = pd.DataFrame(source,columns=['dt','style','filepath']).sort_values(['dt'])\n",
    "\n",
    "filepaths = list(source[source['style']=='starting-2014-10-18']['filepath'])\n",
    "source[source['style']=='starting-2014-10-18']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filepaths = source[(source['dt']>=datetime(2017,1,1).date())&(source['dt']<datetime(2018,1,1).date())]\n",
    "filepaths = source[source['dt']>=datetime(2018,1,1).date()]['filepath']\n",
    "progress_bar = tqdm(total=len(filepaths))\n",
    "for filepath in filepaths:\n",
    "    raw = pd.read_csv(filepath)\n",
    "    print('Read : {} .'.format(filepath))\n",
    "    raw.columns = [c.strip() for c in raw.columns]\n",
    "    for i,grp in raw.groupby(['C/A','UNIT']):\n",
    "        ca = i[0]\n",
    "        unit = i[1]\n",
    "        filepath = 'data/intermediate/CA-BOOTH_{}-{}.csv'.format(ca,unit)\n",
    "        is_first = not os.path.exists(filepath)\n",
    "        with open(filepath,'a') as f:\n",
    "            grp.to_csv(f,header=is_first,index=False)\n",
    "    progress_bar.update()\n",
    "progress_bar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "filenames = [filename for filename in os.listdir('data/intermediate/') if filename.find('CA-BOOTH_')>-1]\n",
    "progress_bar = tqdm(total=len(filenames))\n",
    "for filename in filenames:\n",
    "    filepath = 'data/intermediate/'+filename\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df.drop_duplicates()\n",
    "    df['timestamp'] = [datetime.strptime(\"{} {}\".format(d,t),\"%m/%d/%Y %H:%M:%S\") for d,t in zip(df['DATE'],df['TIME'])]\n",
    "    \n",
    "    for i,grp in df.groupby(['C/A','UNIT','SCP','STATION','LINENAME','DIVISION']):\n",
    "        grp = grp.sort_values('timestamp')\n",
    "        df1 = grp.iloc[:-1].reset_index(drop=True).copy()\n",
    "        df2 = grp.iloc[1:].reset_index(drop=True).copy()\n",
    "        # Get left (headers and start) columns:\n",
    "        df1 = df1.rename(columns={\n",
    "            'timestamp':'start_timestamp',\n",
    "            'ENTRIES':'start_entries',\n",
    "            'EXITS':'start_exits',\n",
    "        }).drop(['DATE','TIME','DESC'],axis=1)\n",
    "        # Get right (end) columns:\n",
    "        df2 = df2.rename(columns={\n",
    "            'timestamp':'end_timestamp',\n",
    "            'ENTRIES':'end_entries',\n",
    "            'EXITS':'end_exits',\n",
    "        }).drop(['DATE','TIME','DESC','C/A','UNIT','SCP','STATION','LINENAME','DIVISION'],axis=1)\n",
    "        # Merge left and right columns:\n",
    "        df3 = pd.concat([df1,df2],axis=1)\n",
    "        # Calculate deltas:\n",
    "        df3['delta_entries'] = df3['end_entries'] - df3['start_entries']\n",
    "        df3['delta_exits'] = df3['end_exits'] - df3['start_exits']\n",
    "        df3['delta_timestamp'] = df2['end_timestamp']-df3['start_timestamp']\n",
    "        df3['delta_timestamp'] = [delta_ddhhmmss(delta) for delta in df3['delta_timestamp']]\n",
    "        # Append result:\n",
    "        data.append(df3)\n",
    "    \n",
    "    progress_bar.update()\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "data = pd.concat(data,axis=0)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculated data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = lookup_booths.copy()\n",
    "df1 = df1[~pd.isnull(df1['Booth'])]\n",
    "df1 = df1[['Complex ID','Booth']].drop_duplicates()\n",
    "df1['Complex ID'] = [-1 if pd.isnull(x) else int(x) for x in df1['Complex ID']]\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = data.copy()\n",
    "df2['delta_entries'] = df2['delta_entries'].abs()\n",
    "df2['delta_exits'] = df2['delta_exits'].abs()\n",
    "df2 = remove_outliers(data=df2,grp_cols=['C/A','UNIT','SCP'],data_cols=['delta_entries','delta_exits'])\n",
    "df2 = df2.groupby(['C/A','UNIT','start_timestamp','end_timestamp','delta_timestamp'])[['delta_entries','delta_exits']].sum()\n",
    "df2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA = df1.merge(df2.reset_index(),left_on=['Booth'],right_on=['UNIT'],how='outer')\n",
    "#DATA = DATA[DATA['delta_timestamp']==\"+04h00m00s\"]\n",
    "DATA['Complex ID'] = [-1 if pd.isnull(x) else int(x) for x in DATA['Complex ID']]\n",
    "DATA['Booth'] = ['?' if pd.isnull(x) else str(x) for x in DATA['Booth']]\n",
    "DATA = DATA.groupby(['Complex ID','start_timestamp','end_timestamp','delta_timestamp'])[['delta_entries','delta_exits']].sum()\n",
    "DATA['delta_entries'] = [int(x) for x in DATA['delta_entries']]\n",
    "DATA['delta_exits'] = [int(x) for x in DATA['delta_exits']]\n",
    "DATA = DATA.reset_index(drop=False)\n",
    "DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data[['delta_entries','delta_exits']].sum()\n",
    "#clean[['delta_entries','delta_exits']].sum()\n",
    "#df2[['delta_entries','delta_exits']].sum()\n",
    "#DATA[['delta_entries','delta_exits']].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA.to_csv('data/results/turnstile_data.csv',index=False)\n",
    "print('Saved : data/results/turnstile_data.csv .')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
